{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from keras.layers import Dense, LSTM\n",
    "from keras.models import Model\n",
    "from keras.engine.input_layer import Input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#                   NAS PARAMETERS                     #\n",
    "########################################################\n",
    "CONTROLLER_SAMPLING_EPOCHS = 10\n",
    "SAMPLES_PER_CONTROLLER_EPOCH = 10\n",
    "CONTROLLER_TRAINING_EPOCHS = 10\n",
    "ARCHITECTURE_TRAINING_EPOCHS = 10\n",
    "CONTROLLER_LOSS_ALPHA = 0.9\n",
    "\n",
    "########################################################\n",
    "#               CONTROLLER PARAMETERS                  #\n",
    "########################################################\n",
    "CONTROLLER_LSTM_DIM = 100\n",
    "CONTROLLER_OPTIMIZER = 'Adam'\n",
    "CONTROLLER_LEARNING_RATE = 0.01\n",
    "CONTROLLER_DECAY = 0.1\n",
    "CONTROLLER_MOMENTUM = 0.0\n",
    "CONTROLLER_USE_PREDICTOR = True\n",
    "\n",
    "########################################################\n",
    "#                   MLP PARAMETERS                     #\n",
    "########################################################\n",
    "MAX_ARCHITECTURE_LENGTH = 3\n",
    "MLP_OPTIMIZER = 'Adam'\n",
    "MLP_LEARNING_RATE = 0.01\n",
    "MLP_DECAY = 0.0\n",
    "MLP_MOMENTUM = 0.0\n",
    "MLP_DROPOUT = 0.2\n",
    "MLP_LOSS_FUNCTION = 'categorical_crossentropy'\n",
    "MLP_ONE_SHOT = True\n",
    "\n",
    "########################################################\n",
    "#                   DATA PARAMETERS                    #\n",
    "########################################################\n",
    "TARGET_CLASSES = 3\n",
    "\n",
    "########################################################\n",
    "#                  OUTPUT PARAMETERS                   #\n",
    "########################################################\n",
    "TOP_N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPSearchSpace(object):\n",
    "\n",
    "    def __init__(self, target_classes):\n",
    "\n",
    "        self.target_classes = target_classes\n",
    "        self.vocab = self.vocab_dict()\n",
    "\n",
    "    def vocab_dict(self):\n",
    "        nodes = [8, 16, 32, 64, 128, 256, 512]\n",
    "        act_funcs = ['sigmoid', 'tanh', 'relu', 'elu']\n",
    "        layer_params = []\n",
    "        layer_id = []\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(len(act_funcs)):\n",
    "                layer_params.append((nodes[i], act_funcs[j]))\n",
    "                layer_id.append(len(act_funcs) * i + j + 1)\n",
    "        vocab = dict(zip(layer_id, layer_params))\n",
    "        vocab[len(vocab) + 1] = (('dropout'))\n",
    "        if self.target_classes == 2:\n",
    "            vocab[len(vocab) + 1] = (self.target_classes - 1, 'sigmoid')\n",
    "        else:\n",
    "            vocab[len(vocab) + 1] = (self.target_classes, 'softmax')\n",
    "        return vocab\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        keys = list(self.vocab.keys())\n",
    "        values = list(self.vocab.values())\n",
    "        encoded_sequence = []\n",
    "        for value in sequence:\n",
    "            encoded_sequence.append(keys[values.index(value)])\n",
    "        return encoded_sequence\n",
    "\n",
    "    def decode_sequence(self, sequence):\n",
    "        keys = list(self.vocab.keys())\n",
    "        values = list(self.vocab.values())\n",
    "        decoded_sequence = []\n",
    "        for key in sequence:\n",
    "            decoded_sequence.append(values[keys.index(key)])\n",
    "        return decoded_sequence\n",
    "\n",
    "\n",
    "class MLPGenerator(MLPSearchSpace):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.target_classes = TARGET_CLASSES\n",
    "        self.mlp_optimizer = MLP_OPTIMIZER\n",
    "        self.mlp_lr = MLP_LEARNING_RATE\n",
    "        self.mlp_decay = MLP_DECAY\n",
    "        self.mlp_momentum = MLP_MOMENTUM\n",
    "        self.mlp_dropout = MLP_DROPOUT\n",
    "        self.mlp_loss_func = MLP_LOSS_FUNCTION\n",
    "        self.mlp_one_shot = MLP_ONE_SHOT\n",
    "        self.metrics = ['accuracy']\n",
    "\n",
    "        super().__init__(TARGET_CLASSES)\n",
    "\n",
    "\n",
    "        if self.mlp_one_shot:\n",
    "            self.weights_file = 'LOGS/shared_weights.pkl'\n",
    "            self.shared_weights = pd.DataFrame({'bigram_id': [], 'weights': []})\n",
    "            if not os.path.exists(self.weights_file):\n",
    "                print(\"Initializing shared weights dictionary...\")\n",
    "                self.shared_weights.to_pickle(self.weights_file)\n",
    "\n",
    "    def create_model(self, sequence, mlp_input_shape):\n",
    "        layer_configs = self.decode_sequence(sequence)\n",
    "        model = Sequential()\n",
    "        if len(mlp_input_shape) > 1:\n",
    "            model.add(Flatten(name='flatten', input_shape=mlp_input_shape))\n",
    "            for i, layer_conf in enumerate(layer_configs):\n",
    "                if layer_conf is 'dropout':\n",
    "                    model.add(Dropout(self.mlp_dropout, name='dropout'))\n",
    "                else:\n",
    "                    model.add(Dense(units=layer_conf[0], activation=layer_conf[1]))\n",
    "        else:\n",
    "            for i, layer_conf in enumerate(layer_configs):\n",
    "                if i == 0:\n",
    "                    model.add(Dense(units=layer_conf[0], activation=layer_conf[1], input_shape=mlp_input_shape))\n",
    "                elif layer_conf is 'dropout':\n",
    "                    model.add(Dropout(self.mlp_dropout, name='dropout'))\n",
    "                else:\n",
    "                    model.add(Dense(units=layer_conf[0], activation=layer_conf[1]))\n",
    "        return model\n",
    "\n",
    "    def compile_model(self, model):\n",
    "        if self.mlp_optimizer == 'sgd':\n",
    "            optim = optimizers.SGD(lr=self.mlp_lr, decay=self.mlp_decay, momentum=self.mlp_momentum)\n",
    "        else:\n",
    "            optim = getattr(optimizers, self.mlp_optimizer)(lr=self.mlp_lr, decay=self.mlp_decay)\n",
    "        model.compile(loss=self.mlp_loss_func, optimizer=optim, metrics=self.metrics)\n",
    "        return model\n",
    "\n",
    "    def update_weights(self, model):\n",
    "        layer_configs = ['input']\n",
    "        for layer in model.layers:\n",
    "            if 'flatten' in layer.name:\n",
    "                layer_configs.append(('flatten'))\n",
    "            elif 'dropout' not in layer.name:\n",
    "                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
    "        config_ids = []\n",
    "        for i in range(1, len(layer_configs)):\n",
    "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
    "        j = 0\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if 'dropout' not in layer.name:\n",
    "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                bigram_ids = self.shared_weights['bigram_id'].values\n",
    "                search_index = []\n",
    "                for i in range(len(bigram_ids)):\n",
    "                    if config_ids[j] == bigram_ids[i]:\n",
    "                        search_index.append(i)\n",
    "                if len(search_index) == 0:\n",
    "                    self.shared_weights = self.shared_weights.append({'bigram_id': config_ids[j],\n",
    "                                                                      'weights': layer.get_weights()},\n",
    "                                                                     ignore_index=True)\n",
    "                else:\n",
    "                    self.shared_weights.at[search_index[0], 'weights'] = layer.get_weights()\n",
    "                j += 1\n",
    "        self.shared_weights.to_pickle(self.weights_file)\n",
    "\n",
    "    def set_model_weights(self, model):\n",
    "        layer_configs = ['input']\n",
    "        for layer in model.layers:\n",
    "            if 'flatten' in layer.name:\n",
    "                layer_configs.append(('flatten'))\n",
    "            elif 'dropout' not in layer.name:\n",
    "                layer_configs.append((layer.get_config()['units'], layer.get_config()['activation']))\n",
    "        config_ids = []\n",
    "        for i in range(1, len(layer_configs)):\n",
    "            config_ids.append((layer_configs[i - 1], layer_configs[i]))\n",
    "        j = 0\n",
    "        for i, layer in enumerate(model.layers):\n",
    "            if 'dropout' not in layer.name:\n",
    "                warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "                bigram_ids = self.shared_weights['bigram_id'].values\n",
    "                search_index = []\n",
    "                for i in range(len(bigram_ids)):\n",
    "                    if config_ids[j] == bigram_ids[i]:\n",
    "                        search_index.append(i)\n",
    "                if len(search_index) > 0:\n",
    "                    print(\"Transferring weights for layer:\", config_ids[j])\n",
    "                    layer.set_weights(self.shared_weights['weights'].values[search_index[0]])\n",
    "                j += 1\n",
    "\n",
    "    def train_model(self, model, x_data, y_data, nb_epochs, validation_split=0.1, callbacks=None):\n",
    "        if self.mlp_one_shot:\n",
    "            self.set_model_weights(model)\n",
    "            history = model.fit(x_data,\n",
    "                                y_data,\n",
    "                                epochs=nb_epochs,\n",
    "                                validation_split=validation_split,\n",
    "                                callbacks=callbacks,\n",
    "                                verbose=0)\n",
    "            self.update_weights(model)\n",
    "        else:\n",
    "            history = model.fit(x_data,\n",
    "                                y_data,\n",
    "                                epochs=nb_epochs,\n",
    "                                validation_split=validation_split,\n",
    "                                callbacks=callbacks,\n",
    "                                verbose=0)\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Controller(MLPSearchSpace):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.max_len = MAX_ARCHITECTURE_LENGTH\n",
    "        self.controller_lstm_dim = CONTROLLER_LSTM_DIM\n",
    "        self.controller_optimizer = CONTROLLER_OPTIMIZER\n",
    "        self.controller_lr = CONTROLLER_LEARNING_RATE\n",
    "        self.controller_decay = CONTROLLER_DECAY\n",
    "        self.controller_momentum = CONTROLLER_MOMENTUM\n",
    "        self.use_predictor = CONTROLLER_USE_PREDICTOR\n",
    "\n",
    "        self.controller_weights = 'LOGS/controller_weights.h5'\n",
    "\n",
    "        self.seq_data = []\n",
    "\n",
    "        super().__init__(TARGET_CLASSES)\n",
    "\n",
    "        self.controller_classes = len(self.vocab) + 1\n",
    "\n",
    "    def sample_architecture_sequences(self, model, number_of_samples):\n",
    "        final_layer_id = len(self.vocab)\n",
    "        dropout_id = final_layer_id - 1\n",
    "        vocab_idx = [0] + list(self.vocab.keys())\n",
    "        samples = []\n",
    "        print(\"GENERATING ARCHITECTURE SAMPLES...\")\n",
    "        print('------------------------------------------------------')\n",
    "        while len(samples) < number_of_samples:\n",
    "            seed = []\n",
    "            while len(seed) < self.max_len:\n",
    "                sequence = pad_sequences([seed], maxlen=self.max_len - 1, padding='post')\n",
    "                sequence = sequence.reshape(1, 1, self.max_len - 1)\n",
    "                if self.use_predictor:\n",
    "                    (probab, _) = model.predict(sequence)\n",
    "                else:\n",
    "                    probab = model.predict(sequence)\n",
    "                probab = probab[0][0]\n",
    "                next = np.random.choice(vocab_idx, size=1, p=probab)[0]\n",
    "                if next == dropout_id and len(seed) == 0:\n",
    "                    continue\n",
    "                if next == final_layer_id and len(seed) == 0:\n",
    "                    continue\n",
    "                if next == final_layer_id:\n",
    "                    seed.append(next)\n",
    "                    break\n",
    "                if len(seed) == self.max_len - 1:\n",
    "                    seed.append(final_layer_id)\n",
    "                    break\n",
    "                if not next == 0:\n",
    "                    seed.append(next)\n",
    "            if seed not in self.seq_data:\n",
    "                samples.append(seed)\n",
    "                self.seq_data.append(seed)\n",
    "        return samples\n",
    "\n",
    "    def control_model(self, controller_input_shape, controller_batch_size):\n",
    "        main_input = Input(shape=controller_input_shape, batch_shape=controller_batch_size, name='main_input')\n",
    "        x = LSTM(self.controller_lstm_dim, return_sequences=True)(main_input)\n",
    "        main_output = Dense(self.controller_classes, activation='softmax', name='main_output')(x)\n",
    "        model = Model(inputs=[main_input], outputs=[main_output])\n",
    "        return model\n",
    "\n",
    "    def train_control_model(self, model, x_data, y_data, loss_func, controller_batch_size, nb_epochs):\n",
    "        if self.controller_optimizer == 'sgd':\n",
    "            optim = optimizers.SGD(lr=self.controller_lr, decay=self.controller_decay, momentum=self.controller_momentum, clipnorm=1.0)\n",
    "        else:\n",
    "            optim = getattr(optimizers, self.controller_optimizer)(lr=self.controller_lr, decay=self.controller_decay, clipnorm=1.0)\n",
    "        model.compile(optimizer=optim, loss={'main_output': loss_func})\n",
    "        if os.path.exists(self.controller_weights):\n",
    "            model.load_weights(self.controller_weights)\n",
    "        print(\"TRAINING CONTROLLER...\")\n",
    "        model.fit({'main_input': x_data},\n",
    "                  {'main_output': y_data.reshape(len(y_data), 1, self.controller_classes)},\n",
    "                  epochs=nb_epochs,\n",
    "                  batch_size=controller_batch_size,\n",
    "                  verbose=0)\n",
    "        model.save_weights(self.controller_weights)\n",
    "\n",
    "    def hybrid_control_model(self, controller_input_shape, controller_batch_size):\n",
    "        main_input = Input(shape=controller_input_shape, batch_shape=controller_batch_size, name='main_input')\n",
    "        x = LSTM(self.controller_lstm_dim, return_sequences=True)(main_input)\n",
    "        predictor_output = Dense(1, activation='sigmoid', name='predictor_output')(x)\n",
    "        main_output = Dense(self.controller_classes, activation='softmax', name='main_output')(x)\n",
    "        model = Model(inputs=[main_input], outputs=[main_output, predictor_output])\n",
    "        return model\n",
    "\n",
    "    def train_hybrid_model(self, model, x_data, y_data, pred_target, loss_func, controller_batch_size, nb_epochs):\n",
    "        if self.controller_optimizer == 'sgd':\n",
    "            optim = optimizers.SGD(lr=self.controller_lr, decay=self.controller_decay, momentum=self.controller_momentum, clipnorm=1.0)\n",
    "        else:\n",
    "            optim = getattr(optimizers, self.controller_optimizer)(lr=self.controller_lr, decay=self.controller_decay, clipnorm=1.0)\n",
    "        model.compile(optimizer=optim,\n",
    "                      loss={'main_output': loss_func, 'predictor_output': 'mse'},\n",
    "                      loss_weights={'main_output': 1, 'predictor_output': 1})\n",
    "        if os.path.exists(self.controller_weights):\n",
    "            model.load_weights(self.controller_weights)\n",
    "        print(\"TRAINING CONTROLLER...\")\n",
    "        model.fit({'main_input': x_data},\n",
    "                  {'main_output': y_data.reshape(len(y_data), 1, self.controller_classes),\n",
    "                   'predictor_output': np.array(pred_target).reshape(len(pred_target), 1, 1)},\n",
    "                  epochs=nb_epochs,\n",
    "                  batch_size=controller_batch_size,\n",
    "                  verbose=0)\n",
    "        model.save_weights(self.controller_weights)\n",
    "\n",
    "    def get_predicted_accuracies_hybrid_model(self, model, seqs):\n",
    "        pred_accuracies = []\n",
    "        for seq in seqs:\n",
    "            control_sequences = pad_sequences([seq], maxlen=self.max_len, padding='post')\n",
    "            xc = control_sequences[:, :-1].reshape(len(control_sequences), 1, self.max_len - 1)\n",
    "            (_, pred_accuracy) = [x[0][0] for x in model.predict(xc)]\n",
    "            pred_accuracies.append(pred_accuracy[0])\n",
    "        return pred_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import keras.backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPNAS(Controller):\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.target_classes = TARGET_CLASSES\n",
    "        self.controller_sampling_epochs = CONTROLLER_SAMPLING_EPOCHS\n",
    "        self.samples_per_controller_epoch = SAMPLES_PER_CONTROLLER_EPOCH\n",
    "        self.controller_train_epochs = CONTROLLER_TRAINING_EPOCHS\n",
    "        self.architecture_train_epochs = ARCHITECTURE_TRAINING_EPOCHS\n",
    "        self.controller_loss_alpha = CONTROLLER_LOSS_ALPHA\n",
    "\n",
    "        self.data = []\n",
    "        self.nas_data_log = 'LOGS/nas_data.pkl'\n",
    "        clean_log()\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_generator = MLPGenerator()\n",
    "\n",
    "        self.controller_batch_size = len(self.data)\n",
    "        self.controller_input_shape = (1, MAX_ARCHITECTURE_LENGTH - 1)\n",
    "        if self.use_predictor:\n",
    "            self.controller_model = self.hybrid_control_model(self.controller_input_shape, self.controller_batch_size)\n",
    "        else:\n",
    "            self.controller_model = self.control_model(self.controller_input_shape, self.controller_batch_size)\n",
    "\n",
    "    def create_architecture(self, sequence):\n",
    "        if self.target_classes == 2:\n",
    "            self.model_generator.loss_func = 'binary_crossentropy'\n",
    "        model = self.model_generator.create_model(sequence, np.shape(self.x[0]))\n",
    "        model = self.model_generator.compile_model(model)\n",
    "        return model\n",
    "\n",
    "    def train_architecture(self, model):\n",
    "        x, y = unison_shuffled_copies(self.x, self.y)\n",
    "        history = self.model_generator.train_model(model, x, y, self.architecture_train_epochs)\n",
    "        return history\n",
    "\n",
    "    def append_model_metrics(self, sequence, history, pred_accuracy=None):\n",
    "        if len(history.history['val_acc']) == 1:\n",
    "            if pred_accuracy:\n",
    "                self.data.append([sequence,\n",
    "                                  history.history['val_acc'][0],\n",
    "                                  pred_accuracy])\n",
    "            else:\n",
    "                self.data.append([sequence,\n",
    "                                  history.history['val_acc'][0]])\n",
    "            print('validation accuracy: ', history.history['val_accuracy'][0])\n",
    "        else:\n",
    "            val_acc = np.ma.average(history.history['val_acc'],\n",
    "                                    weights=np.arange(1, len(history.history['val_acc']) + 1),\n",
    "                                    axis=-1)\n",
    "            if pred_accuracy:\n",
    "                self.data.append([sequence,\n",
    "                                  val_acc,\n",
    "                                  pred_accuracy])\n",
    "            else:\n",
    "                self.data.append([sequence,\n",
    "                                  val_acc])\n",
    "            print('validation accuracy: ', val_acc)\n",
    "\n",
    "    def prepare_controller_data(self, sequences):\n",
    "        controller_sequences = pad_sequences(sequences, maxlen=self.max_len, padding='post')\n",
    "        xc = controller_sequences[:, :-1].reshape(len(controller_sequences), 1, self.max_len - 1)\n",
    "        yc = to_categorical(controller_sequences[:, -1], self.controller_classes)\n",
    "        val_acc_target = [item[1] for item in self.data]\n",
    "        return xc, yc, val_acc_target\n",
    "\n",
    "    def get_discounted_reward(self, rewards):\n",
    "        discounted_r = np.zeros_like(rewards, dtype=np.float32)\n",
    "        for t in range(len(rewards)):\n",
    "            running_add = 0.\n",
    "            exp = 0.\n",
    "            for r in rewards[t:]:\n",
    "                running_add += self.controller_loss_alpha**exp * r\n",
    "                exp += 1\n",
    "            discounted_r[t] = running_add\n",
    "        discounted_r = (discounted_r - discounted_r.mean()) / discounted_r.std()\n",
    "        return discounted_r\n",
    "\n",
    "    def custom_loss(self, target, output):\n",
    "        baseline = 0.5\n",
    "        reward = np.array([item[1] - baseline for item in self.data[-self.samples_per_controller_epoch:]]).reshape(\n",
    "            self.samples_per_controller_epoch, 1)\n",
    "        discounted_reward = self.get_discounted_reward(reward)\n",
    "        loss = - K.log(output) * discounted_reward[:, None]\n",
    "        return loss\n",
    "\n",
    "    def train_controller(self, model, x, y, pred_accuracy=None):\n",
    "        if self.use_predictor:\n",
    "            self.train_hybrid_model(model,\n",
    "                                    x,\n",
    "                                    y,\n",
    "                                    pred_accuracy,\n",
    "                                    self.custom_loss,\n",
    "                                    len(self.data),\n",
    "                                    self.controller_train_epochs)\n",
    "        else:\n",
    "            self.train_control_model(model,\n",
    "                                     x,\n",
    "                                     y,\n",
    "                                     self.custom_loss,\n",
    "                                     len(self.data),\n",
    "                                     self.controller_train_epochs)\n",
    "\n",
    "    def search(self):\n",
    "        for controller_epoch in range(self.controller_sampling_epochs):\n",
    "            print('------------------------------------------------------------------')\n",
    "            print('                       CONTROLLER EPOCH: {}'.format(controller_epoch))\n",
    "            print('------------------------------------------------------------------')\n",
    "            sequences = self.sample_architecture_sequences(self.controller_model, self.samples_per_controller_epoch)\n",
    "            if self.use_predictor:\n",
    "                pred_accuracies = self.get_predicted_accuracies_hybrid_model(self.controller_model, sequences)\n",
    "            for i, sequence in enumerate(sequences):\n",
    "                print('Architecture: ', self.decode_sequence(sequence))\n",
    "                model = self.create_architecture(sequence)\n",
    "                history = self.train_architecture(model)\n",
    "                if self.use_predictor:\n",
    "                    self.append_model_metrics(sequence, history, pred_accuracies[i])\n",
    "                else:\n",
    "                    self.append_model_metrics(sequence, history)\n",
    "                print('------------------------------------------------------')\n",
    "            xc, yc, val_acc_target = self.prepare_controller_data(sequences)\n",
    "            self.train_controller(self.controller_model,\n",
    "                                  xc,\n",
    "                                  yc,\n",
    "                                  val_acc_target[-self.samples_per_controller_epoch:])\n",
    "        with open(self.nas_data_log, 'wb') as f:\n",
    "            pickle.dump(self.data, f)\n",
    "        log_event()\n",
    "        return self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pickle\n",
    "import numpy as np\n",
    "from itertools import groupby\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "#                   DATA PROCESSING                    #\n",
    "########################################################\n",
    "\n",
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "########################################################\n",
    "#                       LOGGING                        #\n",
    "########################################################\n",
    "\n",
    "\n",
    "def clean_log():\n",
    "    filelist = os.listdir('LOGS')\n",
    "    for file in filelist:\n",
    "        if os.path.isfile('LOGS/{}'.format(file)):\n",
    "            os.remove('LOGS/{}'.format(file))\n",
    "\n",
    "\n",
    "def log_event():\n",
    "    dest = 'LOGS'\n",
    "    while os.path.exists(dest):\n",
    "        dest = 'LOGS/event{}'.format(np.random.randint(10000))\n",
    "    os.mkdir(dest)\n",
    "    filelist = os.listdir('LOGS')\n",
    "    for file in filelist:\n",
    "        if os.path.isfile('LOGS/{}'.format(file)):\n",
    "            shutil.move('LOGS/{}'.format(file),dest)\n",
    "\n",
    "\n",
    "def get_latest_event_id():\n",
    "    all_subdirs = ['LOGS/' + d for d in os.listdir('LOGS') if os.path.isdir('LOGS/' + d)]\n",
    "    latest_subdir = max(all_subdirs, key=os.path.getmtime)\n",
    "    return int(latest_subdir.replace('LOGS/event', ''))\n",
    "\n",
    "\n",
    "########################################################\n",
    "#                 RESULTS PROCESSING                   #\n",
    "########################################################\n",
    "\n",
    "\n",
    "def load_nas_data():\n",
    "    event = get_latest_event_id()\n",
    "    data_file = 'LOGS/event{}/nas_data.pkl'.format(event)\n",
    "    with open(data_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def sort_search_data(nas_data):\n",
    "    val_accs = [item[1] for item in nas_data]\n",
    "    sorted_idx = np.argsort(val_accs)[::-1]\n",
    "    nas_data = [nas_data[x] for x in sorted_idx]\n",
    "    return nas_data\n",
    "\n",
    "########################################################\n",
    "#                EVALUATION AND PLOTS                  #\n",
    "########################################################\n",
    "\n",
    "def get_top_n_architectures(n):\n",
    "    data = load_nas_data()\n",
    "    data = sort_search_data(data)\n",
    "    search_space = MLPSearchSpace(TARGET_CLASSES)\n",
    "    print('Top {} Architectures:'.format(n))\n",
    "    for seq_data in data[:n]:\n",
    "        print('Architecture', search_space.decode_sequence(seq_data[0]))\n",
    "        print('Validation Accuracy:', seq_data[1])\n",
    "\n",
    "\n",
    "def get_nas_accuracy_plot():\n",
    "    data = load_nas_data()\n",
    "    accuracies = [x[1] for x in data]\n",
    "    plt.plot(np.arange(len(data)), accuracies)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_accuracy_distribution():\n",
    "    event = get_latest_event_id()\n",
    "    data = load_nas_data()\n",
    "    accuracies = [x[1]*100. for x in data]\n",
    "    accuracies = [int(x) for x in accuracies]\n",
    "    sorted_accs = np.sort(accuracies)\n",
    "    count_dict = {k: len(list(v)) for k, v in groupby(sorted_accs)}\n",
    "    plt.bar(list(count_dict.keys()), list(count_dict.values()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing shared weights dictionary...\n",
      "WARNING:tensorflow:From c:\\users\\puneet rathore\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\puneet rathore\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\puneet rathore\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "------------------------------------------------------------------\n",
      "                       CONTROLLER EPOCH: 0\n",
      "------------------------------------------------------------------\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "WARNING:tensorflow:From c:\\users\\puneet rathore\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\puneet rathore\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "Architecture:  [(64, 'sigmoid'), (8, 'sigmoid'), (3, 'softmax')]\n",
      "WARNING:tensorflow:From c:\\users\\puneet rathore\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\puneet rathore\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "validation accuracy:  0.6551020410596108\n",
      "------------------------------------------------------\n",
      "Architecture:  [(32, 'tanh'), (256, 'sigmoid'), (3, 'softmax')]\n",
      "validation accuracy:  0.6381818184251025\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'relu'), (64, 'sigmoid'), (3, 'softmax')]\n",
      "validation accuracy:  0.6469387759967725\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'elu'), (128, 'elu'), (3, 'softmax')]\n",
      "validation accuracy:  0.3469387756318462\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'sigmoid'), (256, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((256, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.6\n",
      "------------------------------------------------------\n",
      "Architecture:  [(8, 'sigmoid'), (32, 'elu'), (3, 'softmax')]\n",
      "validation accuracy:  0.6102040821192216\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'relu'), (512, 'sigmoid'), (3, 'softmax')]\n",
      "validation accuracy:  0.6240445266583855\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'elu'), (16, 'elu'), (3, 'softmax')]\n",
      "validation accuracy:  0.32448979616165163\n",
      "------------------------------------------------------\n",
      "Architecture:  [(32, 'relu'), (16, 'relu'), (3, 'softmax')]\n",
      "validation accuracy:  0.6346938780375888\n",
      "------------------------------------------------------\n",
      "Architecture:  [(8, 'relu'), (128, 'sigmoid'), (3, 'softmax')]\n",
      "validation accuracy:  0.5954730983479346\n",
      "------------------------------------------------------\n",
      "TRAINING CONTROLLER...\n",
      "------------------------------------------------------------------\n",
      "                       CONTROLLER EPOCH: 1\n",
      "------------------------------------------------------------------\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'tanh'), (3, 'softmax')]\n",
      "validation accuracy:  0.6208905377901109\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'relu'), (32, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (128, 'relu'))\n",
      "validation accuracy:  0.6020408168130992\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (128, 'relu'))\n",
      "validation accuracy:  0.3081632653061225\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'elu'), (8, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (64, 'elu'))\n",
      "validation accuracy:  0.32653061224489793\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'tanh'), (16, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((16, 'relu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6265306123665401\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'elu'), (64, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((64, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.6204081635085904\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'tanh'), (512, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (16, 'tanh'))\n",
      "validation accuracy:  0.659406308198904\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'relu'), (128, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (128, 'relu'))\n",
      "Transferring weights for layer: ((128, 'relu'), (3, 'softmax'))\n",
      "validation accuracy:  0.32857142881471285\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'elu'), (32, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((32, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6571428572644992\n",
      "------------------------------------------------------\n",
      "Architecture:  [(8, 'tanh'), (512, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((512, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.6016326530634362\n",
      "------------------------------------------------------\n",
      "TRAINING CONTROLLER...\n",
      "------------------------------------------------------------------\n",
      "                       CONTROLLER EPOCH: 2\n",
      "------------------------------------------------------------------\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'sigmoid'), (8, 'elu'), (3, 'softmax')]\n",
      "validation accuracy:  0.6484972170686456\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'relu'), 'dropout', (3, 'softmax')]\n",
      "WARNING:tensorflow:From c:\\users\\puneet rathore\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Transferring weights for layer: ('input', (128, 'relu'))\n",
      "Transferring weights for layer: ((128, 'relu'), (3, 'softmax'))\n",
      "validation accuracy:  0.3612244902824869\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'elu'), (64, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (16, 'elu'))\n",
      "validation accuracy:  0.6204081635085904\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'relu'), (32, 'tanh'), (3, 'softmax')]\n",
      "validation accuracy:  0.616326530733887\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'tanh'), (512, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (64, 'tanh'))\n",
      "validation accuracy:  0.6163265310988134\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'relu'), (256, 'tanh'), (3, 'softmax')]\n",
      "validation accuracy:  0.6367346936342668\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'sigmoid'), (256, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (16, 'sigmoid'))\n",
      "validation accuracy:  0.6510204084065496\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'sigmoid'), (256, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (64, 'sigmoid'))\n",
      "Transferring weights for layer: ((256, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.6265306122448979\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'sigmoid'), (512, 'relu'), (3, 'softmax')]\n",
      "validation accuracy:  0.5753988870707425\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'relu'), (32, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (64, 'relu'))\n",
      "Transferring weights for layer: ((32, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.6489795915934505\n",
      "------------------------------------------------------\n",
      "TRAINING CONTROLLER...\n",
      "------------------------------------------------------------------\n",
      "                       CONTROLLER EPOCH: 3\n",
      "------------------------------------------------------------------\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Architecture:  [(8, 'relu'), (64, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (8, 'relu'))\n",
      "Transferring weights for layer: ((64, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.5781447128904551\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'tanh'), (8, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((8, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6326530607379212\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'sigmoid'), (16, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (256, 'sigmoid'))\n",
      "Transferring weights for layer: ((16, 'tanh'), (3, 'softmax'))\n",
      "validation accuracy:  0.6204081637518747\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'tanh'), (8, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (256, 'tanh'))\n",
      "Transferring weights for layer: ((8, 'relu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6816326533045088\n",
      "------------------------------------------------------\n",
      "Architecture:  [(32, 'elu'), (32, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((32, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.31836734718205983\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'relu'), (256, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (16, 'relu'))\n",
      "Transferring weights for layer: ((256, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6163265301256765\n",
      "------------------------------------------------------\n",
      "Architecture:  [(8, 'sigmoid'), (128, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (8, 'sigmoid'))\n",
      "Transferring weights for layer: ((128, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6081632655494067\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'relu'), (512, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (128, 'relu'))\n",
      "Transferring weights for layer: ((512, 'tanh'), (3, 'softmax'))\n",
      "validation accuracy:  0.616549164921543\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (64, 'relu'))\n",
      "Transferring weights for layer: ((64, 'relu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6591836734693878\n",
      "------------------------------------------------------\n",
      "Architecture:  [(32, 'sigmoid'), (32, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((32, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6408163265306123\n",
      "------------------------------------------------------\n",
      "TRAINING CONTROLLER...\n",
      "------------------------------------------------------------------\n",
      "                       CONTROLLER EPOCH: 4\n",
      "------------------------------------------------------------------\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Architecture:  [(32, 'relu'), (256, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (32, 'relu'))\n",
      "Transferring weights for layer: ((256, 'tanh'), (3, 'softmax'))\n",
      "validation accuracy:  0.5648979592080018\n",
      "------------------------------------------------------\n",
      "Architecture:  [(512, 'elu'), (256, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((256, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6326530609812056\n",
      "------------------------------------------------------\n",
      "Architecture:  [(512, 'relu'), (128, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((128, 'relu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6224489793485524\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'relu'), (512, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (16, 'relu'))\n",
      "Transferring weights for layer: ((512, 'tanh'), (3, 'softmax'))\n",
      "validation accuracy:  0.6224489798351209\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'relu'), (16, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (128, 'relu'))\n",
      "Transferring weights for layer: ((16, 'relu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6244897959183673\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'sigmoid'), 'dropout', (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (128, 'sigmoid'))\n",
      "Transferring weights for layer: ((128, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.616326530733887\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'elu'), (8, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (16, 'elu'))\n",
      "Transferring weights for layer: ((8, 'relu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6102040813893687\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'relu'), (128, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (256, 'relu'))\n",
      "Transferring weights for layer: ((128, 'relu'), (3, 'softmax'))\n",
      "validation accuracy:  0.644897959670242\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'elu'), (256, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (64, 'elu'))\n",
      "Transferring weights for layer: ((256, 'tanh'), (3, 'softmax'))\n",
      "validation accuracy:  0.5496474953883211\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'elu'), 'dropout', (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (64, 'elu'))\n",
      "validation accuracy:  0.6000000001216421\n",
      "------------------------------------------------------\n",
      "TRAINING CONTROLLER...\n",
      "------------------------------------------------------------------\n",
      "                       CONTROLLER EPOCH: 5\n",
      "------------------------------------------------------------------\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'tanh'), 'dropout', (3, 'softmax')]\n",
      "validation accuracy:  0.615250463824104\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'elu'), (8, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (256, 'elu'))\n",
      "validation accuracy:  0.6265306124881823\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'sigmoid'), (32, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (16, 'sigmoid'))\n",
      "Transferring weights for layer: ((32, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6122448984457521\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'tanh'), (16, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (128, 'tanh'))\n",
      "Transferring weights for layer: ((16, 'tanh'), (3, 'softmax'))\n",
      "validation accuracy:  0.6224489795918366\n",
      "------------------------------------------------------\n",
      "Architecture:  [(32, 'elu'), (256, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (32, 'elu'))\n",
      "Transferring weights for layer: ((256, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.32040816375187464\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'relu'), (64, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (256, 'relu'))\n",
      "Transferring weights for layer: ((64, 'relu'), (3, 'softmax'))\n",
      "validation accuracy:  0.608163265792691\n",
      "------------------------------------------------------\n",
      "Architecture:  [(32, 'sigmoid'), (64, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (32, 'sigmoid'))\n",
      "Transferring weights for layer: ((64, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6387755099607973\n",
      "------------------------------------------------------\n",
      "Architecture:  [(8, 'relu'), (512, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (8, 'relu'))\n",
      "Transferring weights for layer: ((512, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6265306127314664\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'relu'), (512, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (64, 'relu'))\n",
      "Transferring weights for layer: ((512, 'tanh'), (3, 'softmax'))\n",
      "validation accuracy:  0.5208534326292367\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'relu'), (8, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (256, 'relu'))\n",
      "Transferring weights for layer: ((8, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.6469387756318462\n",
      "------------------------------------------------------\n",
      "TRAINING CONTROLLER...\n",
      "------------------------------------------------------------------\n",
      "                       CONTROLLER EPOCH: 6\n",
      "------------------------------------------------------------------\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'elu'), (128, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (256, 'elu'))\n",
      "Transferring weights for layer: ((128, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.3428571428875533\n",
      "------------------------------------------------------\n",
      "Architecture:  [(16, 'elu'), (64, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (16, 'elu'))\n",
      "Transferring weights for layer: ((64, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.34285714310042714\n",
      "------------------------------------------------------\n",
      "Architecture:  [(512, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (512, 'elu'))\n",
      "Transferring weights for layer: ((512, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.618367347425344\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'sigmoid'), (16, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (128, 'sigmoid'))\n",
      "validation accuracy:  0.6346938773077361\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'tanh'), (16, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (128, 'tanh'))\n",
      "Transferring weights for layer: ((16, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6378849719274021\n",
      "------------------------------------------------------\n",
      "Architecture:  [(8, 'elu'), (8, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ((8, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.6591836739559563\n",
      "------------------------------------------------------\n",
      "Architecture:  [(128, 'relu'), (32, 'relu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (128, 'relu'))\n",
      "validation accuracy:  0.6326530614677741\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'sigmoid'), (64, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (256, 'sigmoid'))\n",
      "Transferring weights for layer: ((64, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6020408168130992\n",
      "------------------------------------------------------\n",
      "Architecture:  [(32, 'tanh'), (128, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (32, 'tanh'))\n",
      "Transferring weights for layer: ((128, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.6163265308555291\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'sigmoid'), (16, 'sigmoid'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (64, 'sigmoid'))\n",
      "Transferring weights for layer: ((16, 'sigmoid'), (3, 'softmax'))\n",
      "validation accuracy:  0.61428571404243\n",
      "------------------------------------------------------\n",
      "TRAINING CONTROLLER...\n",
      "------------------------------------------------------------------\n",
      "                       CONTROLLER EPOCH: 7\n",
      "------------------------------------------------------------------\n",
      "GENERATING ARCHITECTURE SAMPLES...\n",
      "------------------------------------------------------\n",
      "Architecture:  [(256, 'tanh'), (128, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (256, 'tanh'))\n",
      "Transferring weights for layer: ((128, 'tanh'), (3, 'softmax'))\n",
      "validation accuracy:  0.6244897961616516\n",
      "------------------------------------------------------\n",
      "Architecture:  [(512, 'relu'), (64, 'tanh'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (512, 'relu'))\n",
      "validation accuracy:  0.6510204086498339\n",
      "------------------------------------------------------\n",
      "Architecture:  [(512, 'relu'), (512, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (512, 'relu'))\n",
      "Transferring weights for layer: ((512, 'elu'), (3, 'softmax'))\n",
      "validation accuracy:  0.6367346939991931\n",
      "------------------------------------------------------\n",
      "Architecture:  [(64, 'elu'), (512, 'elu'), (3, 'softmax')]\n",
      "Transferring weights for layer: ('input', (64, 'elu'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('DATASETS/wine-quality.csv')\n",
    "x = data.drop('quality_label', axis=1, inplace=False).values\n",
    "y = pd.get_dummies(data['quality_label']).values\n",
    "\n",
    "nas_object = MLPNAS(x, y)\n",
    "data = nas_object.search()\n",
    "\n",
    "get_top_n_architectures(TOP_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
