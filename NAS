as the last advanced topic for this week we will talk about neural architecture search another contemporary topic that deals with searching for the right neural network architecture for a given problem it's it should straight away appeal to you that this is a very important problem considering the various hyper parameters that one has in designing neural networks to get a better understanding of why we need this we have seen over the duration of this course that accuracy and performance has improved with better design of architectures and this often requires human experts to spend hundreds of hours on arbitrary training and testing and hyperparameter tuning this in fact doesn't even mean that the entire space of possible architectures has been searched exhaustively to find the right architecture often researchers use architectures that were proposed earlier and adapt a few elements to arrive at an architecture for a given problem that brings the question can we adopt a systematic and automatic way of learning high performance model architectures and the solution that we're going to talk about is neural architecture search neural architecture search perhaps not in the same name has been around for some time in very early phases one would use evolutionary algorithms such as say genetic algorithms to be able to design neural network architectures this led to exploration of random search for architectures or even bayesian optimization for architectures and hyperparameter tuning over the last decade over the last two to three years neural  architecture search has emerged as an independent important problem it primarily started with searching for these architectures  using principles from reinforcement learning the general problem set up in nas or neural architecture search is given by you have a search space which defines the space of all possible possible architectures that you want to look for for a given problem you have a search strategy to look for an architecture within the search space often the search space can be extremely large and we need a performance estimation strategy to try to estimate a an architecture's performance using standard training and validation performance could be computationally expensive to be  able to try various kinds of architectures so this also needs to be chosen carefully so you have a search space a search strategy which results in an architecture whose performance is estimated based on the performance estimate the search strategy looks further a newer architecture and this loop continues until a desired performance is met on a consistent basis as we just mentioned the search space of nas methods are extremely important so how is the search space defined one can view neural networks as a function that transforms input variables to output variables through a series of operations so if you looked at neural  networks as computational graphs recall the lecture we had earlier each neural network node represents a tensor and is associated with an operation on its parent nodes i so you can say that the computation at a node k is given by z superscript k is the operation at k of its parent nodes ik what are the operations the operations could be convolutions pooling activation functions or even energy operations like concatenation or element wise addition or simple addition so on and so forth nas space is generally a subspace of this general definition of neural architectures so what kind of search spaces do nas methods use broadly speaking one could divide this into two kinds a global space versus a cell-based search space in a global search space the method allows any kind of an architecture so you have a large degree of freedom in arranging the operations you have a set of allowed operations such as convolutions pooling dense layers global average pooling with different hyper parameter settings like number of filters filter width filter height so on and so forth but there are also a few constraints that are specified for example you may not want to start the neural network with pooling as the first operation you may not want to have dense layers before the first set of convolution operations so on and so forth in global search the issue is that the search space is a bit rigid and it could be impractical to scale and transfer considering the extensiveness  of this search space on the other hand sell by cell based search space which was first introduced in this work in 2008 in cvpr of 2018 they introduced a method called nas net the idea here is that a lot of handcrafted architectures are actually designed with repetitions of specific structures we have seen residual blocks in rest nets this idea is taken forward here by defining a cell structure and constructing a network architecture by repeating this cell structure what is a cell you could look at it as a small directed acyclic graph that represents a transformation or a series of transformations in nasnet which was the method introduced in cvpr of 2018 the method learns two kinds of cells a normal cell where the dimension of the input and the output of that transformation is retained example would be convolution and then a reduction cell where the output feature map has width and height reduced for instance like pooling operations here is an example of a global search space where you could have a chain structured search space as given below where given an input you have a series of operations and the operations have to be searched for from your search space from your universe of operations and another variant is where you have a skip connection in your chain structured search space on the other hand in a cell-based search space an architecture template is defined for example here you see an input which first goes through a normal cell then a reduction cell then a normal cell a reduction cell in a normal cell and softmax in very simple terms you could assume that this normal cell was convolution plus batch norm and reduction cell could have been pooling in a standard alexnet context but now in nassnet each of these cells is searched for within a s within a universe of operations so here is the reduction cell of one of the architectures of nassnet called nas net a where you see five blocks with different kinds of operations that have been mentioned on each block which is obtained after the architecture search as we just mentioned some time ago the space of neural architecture search became popular in 2017 with the introduction of nas through reinforcement learning while we have not covered reinforcement reinforcement learning as a topic in this course we'll speak at a very high level to explain how this nas method works so this method proposes the use of a controller which proposes a child architecture and the controller is implemented as an recurrent neural network or an rnn which outputs a sequence of tokens that configure the network architecture the controller rnn is trained similar to a reinforcement learning task using a popular algorithm called reinforce which uses monte carlo policy gradients the action space in this case is the list of tokens for defining a child network the reward is the accuracy of the child network and the loss is the reinforced loss given by the stub since we have not covered reinforcement learning in this course we won't go further deeper than this but if you are interested you can read this paper for more details here is the schematic you have the controller which samples an architecture with probability b p you train a child network with that architecture to get an accuracy which is here a reward in the context of reinforcement learning and then the policy gradient is computed to update the controller rnn here is an example of how the controller rnn works which is trained using reinforce so you can see that at each step of the rnn across different layers multiple hyper parameters such as number of filters filter height filter width stride height and stride width are predicted for that layer which is then passed on to the next layer whose number of filters filter height and other hyper parameters are predicted so the prediction could be something like 371 to 36  where the first parameter says the filter height the next one gives the filter width the stride height the stride with the number of filters so on and so forth one of the limitations of nasbase methods is that when you view your entire search space as a graph with each path as a certain sequence of operations which could be an architecture each of these paths are looked at and trained independently to be able to find which architecture suits a particular task so given the overall search space the path 123 which could be say a convolution followed by a pooling and a path 1 to 4 which could be convolution followed by a batch norm for instance each of these are trained independently and their performances are assessed to see which of those operations or sequence of operations suits for a given task a recent method called efficient nas proposed in icml of 2018 asks the question can we treat all model trajectories as subgraphs of a single directed acyclic graph and the answer is yes and efficientness shows a way to do this by sharing parameters across the child models so in this case if what you see here is a super graph of all possible sequences of operations each architecture becomes a sub graph or a trajectory in this graph so you could look at these red arrows here as one sequence of operations corresponding to one architecture and that sequence of operations is decided by an rnn controller which is trained using reinforcement learning how is this trained the weights of the controller and that the weights corresponding to the chosen parts are trained in an alternate manner and this leads to the final training and they show that this approach gives about 2.89 percent test error on the safartan dataset which is close to state of the art and took less than 16 hours to search for the right architecture which is significantly lesser than other nas based models especially nas nas-based models based on reinforcement learning can take many days to search for the right architecture one other observation to point out here is a lot of the state-of-the-art models today in classification in detection are all based on architectures obtained through neural architecture search a more recent development called darts or differentiable architecture search was proposed in iclr of 2019 and the key idea here was to define the search space using a set of binary variables on the operations so your output of a layer zk is now an operation o multiplied by a binary variable into in terms of the input variables zi from the previous layer the key insight in this approach is that until this method in all methods you could either have an operation or not have an operation but this method says why don't we relax this and make alpha something that's learnable and hence it relaxes this categorical choice of choosing an operation as a soft max over all possible operations so the operation is not just binary in terms of its existence it's now a soft max that you have over the space of all possible operations this allows us to back propagate through the choice of the operation itself what does this mean how do you train so in darts uh an alternating bi-level optimization method is proposed similar to efficient nas where in the first iteration the model parameters w of a specific architecture is trained by minimizing loss on a training set and in the second step the structural parameters alpha which weight each of your operations are learned using by minimizing another loss on a validation set which is defined this way where w stair star alpha is the minimum loss of w on the train set so you use those weights to be able to get a validation loss which is then used to minimize the value of alpha so the final architecture is chosen based on the sequence of operations that maximizes alpha or the operation at that step that gives you the maximum alpha this is an elegant solution that makes all parameters differentiable and entire and does the entire architecture search as a differentiable procedure other considerations that have been looked at in the nas community is architecture transferability if we learn an architecture through a nas method on sepharton how well does it transfer to imagenet and research has shown that a lot of contemporary nas methods deliver fairly well on this count where an architecture trained on sepharton also does well reasonably on imagenet as well as on other datasets and tasks while this discussion on nas was indented intended to be brief keeping in view the scope of this course let's discuss a few future directions and open problems in nas one of the first issues is search efficiency as i just mentioned trying to perform nas using reinforcement learning can create a training overhead of many days on standard gpu architecture how do you improve the search becomes an important problem a different perspective is to also move towards less constrained search spaces one way you could improve your search efficiency is to constrain your search space more but now we also don't want to constrain the search space more to be able to explore better architectures designing efficient architectures such as those we get after pruning or after quantization or probably finding the lottery ticket using nas could also be an interesting direction of this space another important direction is a joint optimization of all components of a deep learning pipeline not just the architecture we could also talk about what data augmentation to use what activation functions to use what optimizers to use and so on and so forth so including all of that in the nas search pipeline would be an interesting future task in the space finally designing architectures for multimodal tasks such as vision language tasks could also be a very important and useful direction before we conclude this lecture there is also an interesting observation by a few researchers about the curious case of random search in this work interplay i 2019 researchers observed that when one randomly searched for a neural network architecture the difference in accuracy was not too much between using random search reinforcement learning or evolutionary algorithms and this leads to an important conversation as to whether we really need neural architecture search this observation was also seconded by a couple of other papers which showed that random search and a random wiring of neural networks can perform to a reasonable extent in in contrast to what was observed earlier in terms of searching neural network architectures this leaves an interesting and open question for the community the recommended readings for this lecture are once again a very nice blog article by lillian wein on neural architecture search and these two survey papers on neural architecture search if you need more information
